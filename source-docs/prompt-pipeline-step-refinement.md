I want to build and run a few tests of this pipeline. Here's what I want to accomplish. In the task where we built the geographic entity detector, I used that opportunity to test the script on a lof of files and perform other resaerch tasks to build acomprehensive region pattern library stored in MongoDB (pattern_libraries collection, type:"geographic_entity"). Now we have a database driven library for the script to utilize to detect regions in the source titles. I want to use this pipeline script to work through the previous scripts and help use them to build enhanced pattern libraries for each. The goal is that my the time we get to the full pipeline running, we've alread brute forced our way through each individual step an build comprehensive libraries for the scripts to use to help with detection on larger processing runs. So, I want to go step-by-step throught he pipeline and use it to pass data from one script to another when we transition, to deeply evaluate their output and refine them.

For example, here's what I want to do:
1. Run the 01 test script on 500 to 1000 real documents in the database (marktes_raw) and verify that the logic is working to properly separate and classify the document titles by market terms. It will output test files that I can review manually and work with you to refine the script, if necessary, to increase accuracy. Once that is working at a sufficient level, we'll move on to the next script.
2. Use script 01 to process real data from the database and pass market term classified titles to script 02, and then use script 02 to extract the date ranges. WE need to run lots of tests and try enhance the pattern_libraries with all the scenarios of date range presentation we can find until we get the accuracy extremely high. This means running lots of tests, outputting a lot of sample outputs, evaluating them manually and with AI, and improving the script and pattern_libraries until we get it to a high quality point. Then we move on to the enxt script.
3. Use scripts 01 and 02 to process real data from the database and pass market term classified titles to script 03 where we will refine the report type extraction, enhance the pattern_libraries with as many know report type patterns as we can find until we hit a high success rate, and then move on to the next script.
4. Use scripts 01, 02, and 03 to process real data from the database and pass market term classified titles to script 04 where we will test our already enhanced geographic region detection using our robust patter_libraries. We'll enhance more if we need to until we hit a hgihg success rate and then move on to the last script.
5. Use scripts 01, 02, 03, and 04 to process real data from the database and pass market term classified titles to script 05 where we will hopefully have properly extracted all of the other information and we are left with the topic and topicName. We'll need to output sample files, review them, refine if necessary. and once we hit a high success rate.

At that point we will have refined the system and enhanced the refernce libraries.

Confirm that you understand these objective and process and create a comprehensive todo list to accomplish it. Ensure there are todo for testing, evaluation, refinement, fixes, and iterations before moving on to the next step.