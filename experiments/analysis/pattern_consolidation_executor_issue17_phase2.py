#!/usr/bin/env python3
"""
GitHub Issue #17 Phase 2: Pattern Consolidation Executor
Implements evidence-based database consolidation operations

Executes Category C (REMOVE) and Category B (CONSOLIDATE) operations based on analysis results.
"""

import os
import sys
import json
import logging
from datetime import datetime
import pytz
from typing import Dict, List, Any, Optional
from pymongo import MongoClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Setup logging  
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_consolidation_plan(plan_file: str) -> Dict:
    """Load the consolidation plan from analysis results."""
    with open(plan_file, 'r') as f:
        return json.load(f)

def execute_category_c_removals(collection, c_remove_patterns: List[Dict]) -> Dict:
    """
    Execute Category C (REMOVE) operations - safest operation first.
    Remove exact duplicate entries, keeping one canonical entry.
    """
    results = {
        "patterns_processed": 0,
        "documents_removed": 0,
        "errors": []
    }
    
    for pattern_analysis in c_remove_patterns:
        try:
            pattern_regex = pattern_analysis["pattern"]
            documents = pattern_analysis["documents"]
            
            logger.info(f"Processing Category C pattern: {pattern_regex}")
            logger.info(f"Found {len(documents)} duplicate documents")
            
            # Sort documents by creation date to keep the oldest (most established)
            sorted_docs = sorted(documents, key=lambda x: x.get("created_date", datetime.min))
            keep_doc = sorted_docs[0]  # Keep the oldest
            remove_docs = sorted_docs[1:]  # Remove the rest
            
            logger.info(f"Keeping document: {keep_doc['id']} (term: '{keep_doc['term']}')")
            
            # Remove duplicate documents
            for doc_to_remove in remove_docs:
                doc_id = ObjectId(doc_to_remove["id"])
                result = collection.delete_one({"_id": doc_id})
                
                if result.deleted_count == 1:
                    logger.info(f"‚úÖ Removed duplicate: {doc_id} (term: '{doc_to_remove['term']}')")
                    results["documents_removed"] += 1
                else:
                    error_msg = f"Failed to remove document {doc_id}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            results["patterns_processed"] += 1
            
        except Exception as e:
            error_msg = f"Error processing Category C pattern {pattern_regex}: {str(e)}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
    
    return results

def execute_category_b_consolidations(collection, b_consolidate_patterns: List[Dict]) -> Dict:
    """
    Execute Category B (CONSOLIDATE) operations.
    Merge similar patterns with priority resolution (lowest priority wins).
    """
    results = {
        "patterns_processed": 0,
        "documents_consolidated": 0,
        "errors": []
    }
    
    for pattern_analysis in b_consolidate_patterns:
        try:
            pattern_regex = pattern_analysis["pattern"]
            documents = pattern_analysis["documents"]
            
            logger.info(f"Processing Category B pattern: {pattern_regex}")
            logger.info(f"Found {len(documents)} documents to consolidate")
            
            # Determine consolidation strategy
            unique_priorities = pattern_analysis["unique_priorities"]
            target_priority = min(unique_priorities)  # Lowest priority wins
            
            # Find the document with lowest priority (or oldest if tied)
            target_docs = [doc for doc in documents if doc["priority"] == target_priority]
            if len(target_docs) > 1:
                # If multiple docs with same lowest priority, keep oldest
                target_docs = sorted(target_docs, key=lambda x: x.get("created_date", datetime.min))
            
            keep_doc = target_docs[0]
            remove_docs = [doc for doc in documents if doc["id"] != keep_doc["id"]]
            
            logger.info(f"Consolidating to: {keep_doc['id']} (priority: {target_priority}, term: '{keep_doc['term']}')")
            
            # Update the kept document to ensure it has the consolidated properties
            keep_doc_id = ObjectId(keep_doc["id"])
            update_data = {
                "priority": target_priority,
                "active": True,  # Consolidated patterns should be active
                "last_updated": datetime.utcnow(),
                "notes": f"Consolidated from {len(documents)} duplicate entries - GitHub Issue #17 Phase 2"
            }
            
            collection.update_one({"_id": keep_doc_id}, {"$set": update_data})
            logger.info(f"‚úÖ Updated consolidated document with priority {target_priority}")
            
            # Remove the other documents
            for doc_to_remove in remove_docs:
                doc_id = ObjectId(doc_to_remove["id"])
                result = collection.delete_one({"_id": doc_id})
                
                if result.deleted_count == 1:
                    logger.info(f"‚úÖ Removed duplicate: {doc_id} (priority: {doc_to_remove['priority']}, term: '{doc_to_remove['term']}')")
                    results["documents_consolidated"] += 1
                else:
                    error_msg = f"Failed to remove document {doc_id}"
                    logger.error(error_msg)
                    results["errors"].append(error_msg)
            
            results["patterns_processed"] += 1
            
        except Exception as e:
            error_msg = f"Error processing Category B pattern {pattern_regex}: {str(e)}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
    
    return results

def validate_consolidation(collection) -> Dict:
    """Validate that consolidation was successful by checking for remaining duplicates."""
    pipeline = [
        {"$match": {"type": "report_type"}},
        {"$group": {
            "_id": "$pattern",
            "count": {"$sum": 1},
            "terms": {"$addToSet": "$term"}
        }},
        {"$match": {"count": {"$gt": 1}}},
        {"$sort": {"count": -1}}
    ]
    
    remaining_duplicates = list(collection.aggregate(pipeline))
    
    return {
        "remaining_duplicate_patterns": len(remaining_duplicates),
        "total_remaining_documents": sum(dup["count"] for dup in remaining_duplicates),
        "remaining_patterns": remaining_duplicates
    }

def create_output_directory(script_name: str) -> str:
    """Create timestamped output directory."""
    pdt = pytz.timezone('America/Los_Angeles')
    timestamp = datetime.now(pdt).strftime('%Y%m%d_%H%M%S')
    output_dir = f"../outputs/{timestamp}_{script_name}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def export_backup_to_directory(collection, backup_collection_name: str, export_dir: str) -> bool:
    """Export the backup collection to JSON file in specified directory."""
    try:
        db = collection.database
        backup_collection = db[backup_collection_name]
        
        # Export to JSON file
        backup_docs = list(backup_collection.find({}))
        export_file = os.path.join(export_dir, f"{backup_collection_name}.json")
        
        with open(export_file, 'w') as f:
            json.dump(backup_docs, f, indent=2, default=str)
        
        logger.info(f"‚úÖ Backup exported to: {export_file}")
        logger.info(f"üìÅ Backup location: {os.path.abspath(export_file)}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to export backup: {e}")
        return False

def main():
    """Execute GitHub Issue #17 Phase 2 pattern consolidation operations."""
    
    logger.info("Starting GitHub Issue #17 Phase 2: Pattern Consolidation Executor")
    
    # Connect to MongoDB
    try:
        client = MongoClient(os.getenv('MONGODB_URI'))
        db = client['deathstar']
        collection = db['pattern_libraries']
        logger.info("Connected to MongoDB successfully")
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}")
        return
    
    # Create output directory
    output_dir = create_output_directory("issue17_phase2_consolidation_execution")
    logger.info(f"Output directory: {output_dir}")
    
    # Load consolidation plan from analysis results
    plan_file = "../outputs/20250827_214431_issue17_phase2_pattern_consolidation/consolidation_plan.json"
    
    if not os.path.exists(plan_file):
        logger.error(f"Consolidation plan file not found: {plan_file}")
        return
        
    consolidation_plan = load_consolidation_plan(plan_file)
    logger.info(f"Loaded consolidation plan with {consolidation_plan['summary']['total_duplicates']} duplicate patterns")
    
    # Get current timestamps
    pdt = pytz.timezone('America/Los_Angeles')
    utc = pytz.timezone('UTC')
    current_time_pdt = datetime.now(pdt)
    current_time_utc = datetime.now(utc)
    
    # Initialize execution results
    execution_results = {
        "execution_time_pdt": current_time_pdt.strftime('%Y-%m-%d %H:%M:%S %Z'),
        "execution_time_utc": current_time_utc.strftime('%Y-%m-%d %H:%M:%S %Z'),
        "plan_summary": consolidation_plan["summary"],
        "category_c_results": {},
        "category_b_results": {},
        "validation_results": {},
        "overall_success": True
    }
    
    # Execute Category C (REMOVE) operations first - safest
    logger.info("\n" + "="*60)
    logger.info("EXECUTING CATEGORY C (REMOVE) OPERATIONS")
    logger.info("="*60)
    
    c_remove_patterns = consolidation_plan["actions"]["C_REMOVE"]
    if c_remove_patterns:
        c_results = execute_category_c_removals(collection, c_remove_patterns)
        execution_results["category_c_results"] = c_results
        logger.info(f"Category C Results: {c_results['patterns_processed']} patterns, {c_results['documents_removed']} documents removed")
        if c_results["errors"]:
            execution_results["overall_success"] = False
    else:
        logger.info("No Category C patterns to process")
        execution_results["category_c_results"] = {"patterns_processed": 0, "documents_removed": 0, "errors": []}
    
    # Execute Category B (CONSOLIDATE) operations
    logger.info("\n" + "="*60)
    logger.info("EXECUTING CATEGORY B (CONSOLIDATE) OPERATIONS") 
    logger.info("="*60)
    
    b_consolidate_patterns = consolidation_plan["actions"]["B_CONSOLIDATE"]
    if b_consolidate_patterns:
        b_results = execute_category_b_consolidations(collection, b_consolidate_patterns)
        execution_results["category_b_results"] = b_results
        logger.info(f"Category B Results: {b_results['patterns_processed']} patterns, {b_results['documents_consolidated']} documents consolidated")
        if b_results["errors"]:
            execution_results["overall_success"] = False
    else:
        logger.info("No Category B patterns to process")
        execution_results["category_b_results"] = {"patterns_processed": 0, "documents_consolidated": 0, "errors": []}
    
    # Export backup to @backups/database directory
    logger.info("\n" + "="*60)
    logger.info("EXPORTING BACKUP TO @backups/database")
    logger.info("="*60)
    
    backup_export_dir = "../backups/database"
    backup_collection_name = "pattern_libraries_backup_20250827_215350"  # Use existing backup
    export_success = export_backup_to_directory(collection, backup_collection_name, backup_export_dir)
    execution_results["backup_exported"] = export_success
    execution_results["backup_export_location"] = backup_export_dir if export_success else None
    
    # Validate consolidation results
    logger.info("\n" + "="*60)
    logger.info("VALIDATING CONSOLIDATION RESULTS")
    logger.info("="*60)
    
    validation_results = validate_consolidation(collection)
    execution_results["validation_results"] = validation_results
    
    if validation_results["remaining_duplicate_patterns"] > 0:
        logger.warning(f"‚ö†Ô∏è  {validation_results['remaining_duplicate_patterns']} duplicate patterns still remain")
        logger.info("Remaining duplicates are likely Category A (PRESERVE) or Category D (INVESTIGATE)")
    else:
        logger.info("‚úÖ No duplicate patterns remaining - consolidation successful!")
    
    # Save execution results
    results_file = os.path.join(output_dir, "consolidation_execution_results.json")
    with open(results_file, 'w') as f:
        json.dump(execution_results, f, indent=2, default=str)
    logger.info(f"Execution results saved to: {results_file}")
    
    # Generate execution summary report
    total_docs_affected = (execution_results["category_c_results"]["documents_removed"] + 
                          execution_results["category_b_results"]["documents_consolidated"])
    
    summary_report = f"""# GitHub Issue #17 Phase 2: Pattern Consolidation Execution Results

**Execution Date (PDT):** {current_time_pdt.strftime('%Y-%m-%d %H:%M:%S %Z')}  
**Execution Date (UTC):** {current_time_utc.strftime('%Y-%m-%d %H:%M:%S %Z')}

## Consolidation Operations Executed

### Category C (REMOVE) - Exact Duplicates
- **Patterns Processed:** {execution_results["category_c_results"]["patterns_processed"]}
- **Documents Removed:** {execution_results["category_c_results"]["documents_removed"]}
- **Errors:** {len(execution_results["category_c_results"]["errors"])}

### Category B (CONSOLIDATE) - Priority Resolution
- **Patterns Processed:** {execution_results["category_b_results"]["patterns_processed"]}
- **Documents Consolidated:** {execution_results["category_b_results"]["documents_consolidated"]}
- **Errors:** {len(execution_results["category_b_results"]["errors"])}

## Validation Results

- **Remaining Duplicate Patterns:** {validation_results["remaining_duplicate_patterns"]}
- **Total Documents Affected:** {total_docs_affected}
- **Overall Success:** {"‚úÖ YES" if execution_results["overall_success"] else "‚ùå NO"}

## Summary

{"‚úÖ **CONSOLIDATION SUCCESSFUL**" if execution_results["overall_success"] and validation_results["remaining_duplicate_patterns"] <= 4 else "‚ö†Ô∏è **CONSOLIDATION PARTIAL**"}

The remaining {validation_results["remaining_duplicate_patterns"]} duplicate patterns are expected to be Category A (PRESERVE) patterns with legitimate variations or Category D (INVESTIGATE) patterns requiring manual review.

## Database State Changes

**Before Consolidation:** {consolidation_plan["summary"]["total_duplicates"]} patterns with duplicates  
**After Consolidation:** {validation_results["remaining_duplicate_patterns"]} patterns with duplicates  
**Documents Cleaned:** {total_docs_affected} duplicate entries removed/consolidated

## Next Steps

1. ‚úÖ **Category C & B Operations Complete**
2. üîç **Review Category D patterns** for pattern collision resolution  
3. ‚úÖ **Validate Category A patterns** are appropriately preserved
4. üß™ **Run pipeline testing** to verify consolidation impact
5. üìù **Document consolidation outcomes** in GitHub Issue #17

---
**Implementation:** Claude Code AI  
**GitHub Issue:** #17 Phase 2 - Evidence-Based Pattern Consolidation  
**Status:** Database Operations {"Complete" if execution_results["overall_success"] else "Partial"}
"""
    
    summary_file = os.path.join(output_dir, "execution_summary.md")
    with open(summary_file, 'w') as f:
        f.write(summary_report)
    logger.info(f"Execution summary saved to: {summary_file}")
    
    # Final execution summary
    logger.info("\n" + "="*60)
    logger.info("GITHUB ISSUE #17 PHASE 2 EXECUTION COMPLETE")
    logger.info("="*60)
    logger.info(f"Category C: {execution_results['category_c_results']['patterns_processed']} patterns, {execution_results['category_c_results']['documents_removed']} docs removed")
    logger.info(f"Category B: {execution_results['category_b_results']['patterns_processed']} patterns, {execution_results['category_b_results']['documents_consolidated']} docs consolidated")
    logger.info(f"Remaining Duplicates: {validation_results['remaining_duplicate_patterns']} (expected Category A/D)")
    logger.info(f"Overall Success: {'‚úÖ YES' if execution_results['overall_success'] else '‚ùå NO'}")
    logger.info(f"Output Directory: {output_dir}")
    
    client.close()

if __name__ == "__main__":
    main()